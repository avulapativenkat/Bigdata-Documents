Hadoop dfs commands:
================
dfs -ls /hivedemos

Comments in Hive:
==============
-- This is script

Databases in hive:
=================

Default database is default database.

To create database:
===================
create database student;

To suppress already existing database warnings:
===================================
create database IF NOT EXISTS student;

Note: You can also use the keyword SCHEMA instead of database.

To see databases:
===============
Show databases;

Hive ==> create a directory for each database.

Tables in that database will be stored in sub directories of the database directory.

Note: The database directory is created under hive.metastore.warehouse.dir.

default location: /user/hive/warehouse/student.db

To add comment for database:
======================
create database student comment 'Holds all student tables';

To describe database:
====================
describe database student;

Note: Also shows the directory location for the database.

To associate key-value properties with the database:
===================================================
create database student WITH DBPROPERTIES ('creator' = 'subhashini' , 'date' = '2015-1-27');

describe database EXTENDED student;

Use command:
===========
sets a database as your working database.

use student;

Note: there is no command to show you which databse is your current working database.

To drop database:
================
drop database if exists student;


To Alter Database:
=============
Note: You can set key-value pairs in the DBPROPERTIES associated with a databse using Alter Database command. No other metadata about the database can be changed including its name and directory location.

alter database student SET DBPROPERTIES ('edited-by' = 'Subha');

Note: There is no way to delete or unset a DBPROPERTY.

Managing Tables in Hive:
=======================
Two kinds of tables:

1. Managed Table

2. External Table

====================================================================================================================

Dataset: Student.tsv

1001	John	45.0
1002	James	85.0
1003	John	45.0
1004	James	85.0
1005	Smith	60.0
1006	Scott	70.0
1007	Shoba	80.0
1008	Taanu	90.0
1009	Anbu	95.0
1010	Aruna	85.0

Managed Table:
=============

create table IF NOT EXISTS student_int (studID INT COMMENT 'Student ID',studName STRING, gpa FLOAT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

Note: Stored in warehouse folder. When you drop an internal table, it drops the data, and it also drops the metadata.

External Table:
==============

create EXTERNAL table IF NOT EXISTS student_ext (studID INT COMMENT 'Student ID',studName STRING, gpa FLOAT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/student_information';

Note: Create a new directory.

Note: When you drop an external table, it only drops the meta data. That means hive is ignorant of that data now. It does not touch the data itself.

Data Manipualtion in Hive:
=========================
Loading Data to Hive Tables:
===========================
LOAD DATA INPATH '/hivedemos/student.tsv' INTO TABLE student_int;

LOAD DATA LOCAL INPATH '/home/vagrant/bigdata/hivedemos/student.tsv' INTO TABLE student_int;

LOAD DATA LOCAL INPATH '/home/vagrant/bigdata/hivedemos/student.tsv' INTO TABLE student_ext;

Complex Data Types:
==================
student.csv
==========
John Smith,80.0,Joshi:Jack,English!24:EVS!25:Hindi!23
scott Tiger,90.0,James:Aruna,English!25:EVS!25:Hindi!22 
 
create table student_complex(name String,gpa FLOAT, classmates ARRAY<STRING>,marks MAP<STRING,INT>) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY ':'
MAP KEYS TERMINATED BY '!';

Describe student_complex;

LOAD DATA LOCAL INPATH '/home/vagrant/bigdata/hivedemos/student.csv' INTO TABLE student_complex;

select * from student_complex;

select name, classmates from student_complex;

select name, marks from student_complex;

select name, marks['English']  from student_complex;

Order by:
========
SELECT s.studID, s.studName,s.gpa                    
FROM student s
ORDER BY s.studName DESC;

Partition Table:
===============
1. Static Partition 2. Dynamic Partition

Static Partition:
================
Static Partition (SP) columns:The columns whose values are known at COMPILE TIME (given by user).

CREATE TABLE student_sp(studID INT, studName STRING COMMENT 'Student Name')
PARTITIONED BY (gpa FLOAT);

INSERT OVERWRITE TABLE student_sp
PARTITION (gpa=45.0)
select studID,studName from student
where gpa=45.0;

show partitions student_sp;

alter table student_sp add partition(gpa=85.0);

INSERT OVERWRITE TABLE student_sp
PARTITION (gpa=85.0)
select studID,studName from student;


Dynamic Partition:
=================

CREATE TABLE student_dp(studID INT, studName STRING COMMENT 'Student Name')
PARTITIONED BY (gpa FLOAT);

SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

Note: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict

INSERT OVERWRITE TABLE student_dp
PARTITION (gpa)
select studID,studName,gpa from student;

select * from student_dp where gpa=45.0;

show partitions student_dp;

select * from student_dp where gpa=45.0;

Drop Table:
==========

Drop table [IF EXISTS] student;

Buckets:
=======

CREATE TABLE student_bucket(studID INT,studName STRING) PARTITIONED BY (gpa FLOAT) CLUSTERED BY (studID) INTO 2 BUCKETS;

from student
insert overwrite table student_bucket
partition(gpa)
select studID,studName,gpa;

select count (*) from student_bucket;

select studID,studName from student_bucket tablesample(bucket 1 out of 2 on studID) where gpa=45.0;


File Format:
===========

RCFILE Format:[Provides Column Oriented Database Features]
=============
store data in Hadoop file system in column-oriented format using RCFile.

CREATE TABLE STUDENT_RC(studID int,studName string,gpa float) STORED AS RCFILE;

INSERT OVERWRITE table STUDENT_RC SELECT * from STUDENT;

select sum(gpa) from STUDENT_RC;

Sub Query:
=========

docs.txt:

welcome to hadoop session
hadoop definitive guide
hadoop is great technology
hive is a datawarehousing system

Hive Word Count:

CREATE TABLE docs (line STRING);

LOAD DATA LOCAL INPATH '/home/vagrant/hivedemos/docs.txt' OVERWRITE INTO TABLE docs;

SELECT word, count(*) AS count FROM
(SELECT explode (split (line, ' ')) AS word FROM docs) w
GROUP BY word
ORDER BY word;

explode() takes in an array (or a map) as an input and outputs the elements of the array (map) as separate rows.

[welcome to Hadoop]

welcome
to
Hadoop

===================================
where clause:

student.txt:
===========
101,11,Joshi
102,22,Jack
103,44,Smith
104,66,Scott


Department.txt:
==============
11,MCA
22,M.E
33,M.Sc
44,MBA
55,B.E

Create table if not exists student_info (stud_id int,dept_id int,stud_name varchar(20))
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/vagrant/hivedemos/student.txt' INTO TABLE student_info;

Create table if not exists department_info(dept_id int,dept_name String)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/vagrant/hivedemos/department.txt' INTO TABLE department_info;

select s.stud_name from student_info s where s.dept_id in (select dept_id from department_info);

Use of subqueries within WHERE => in HiveQL 0.13. However, they are still limited to IN and EXISTS.
----------------------------------------------------------------------------------------------------
Map-Side Join:
-------------

stud.txt

101,John,1001
102,Jack,1002
103,Smith,1001
104,Scott,1002
105,Joshi,1001
106,Joseph,1002
107,Taanu,1001
108,Aruna,1002
109,Anbu,1001
110,Shoba,1002

college.txt

1001,BMS IT
1002, PES IT

create table student_college(studID INT,studName string,collegeID int) row format delimited fields terminated by ',';

LOAD DATA LOCAL INPATH '/home/vagrant/hivedemos/stud.txt' INTO TABLE student_college;

create table college(collegeID INT,collegeName string) row format delimited fields terminated by ',';

LOAD DATA LOCAL INPATH '/home/vagrant/hivedemos/college.txt' INTO TABLE college;
-------------------------------------------------------------------------------------------

SELECT /*+ MAPJOIN (college) */ student_college.studName, college.collegeName
FROM student_college JOIN college
ON student_college.collegeID=college.collegeID;

**********Normal Join**********

SELECT student_college.studName, college.collegeName
FROM student_college JOIN college
ON student_college.collegeID=college.collegeID;

------------------------------------------------------------------------------------------------
Aggregation:
-----------
count(*)
sum(col)
avg(col)
min(col)
max(col)

Creating Views:
--------------
view support is available only in version starting from 0.6. views are purely logical object.
 	 
Creating View:
=============

create table student(studID int,studName string,gpa float) row format delimited fields terminated by '\t';

load data local inpath '/home/vagrant/hivedemos/student.tsv' into table student;

CREATE VIEW student_view AS SELECT studID,studName FROM student;

Selecting From View:
===============

SELECT * FROM student_view;

Dropping View:
=========== 
DROP VIEW student_view ;

Index:
====
build an index on columns to speed some operations.

creates a seperate table for index.

CREATE INDEX stud_index 
ON TABLE student(studID) 
AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'
WITH DEFERRED REBUILD;

WITH DEFERRED REBUILD portion of the command prevents the index from immediately being built.

CompactIntexHandler ==> a Java class that implements indexing.
 
To build the index you can issue the following command:
=========================================
ALTER INDEX stud_index 
ON student
REBUILD;

describe <<databasename>_<<tablename>>_<<index_name>>;

SHOW FORMATTED INDEX ON student;

DROP INDEX IF EXISTS stud_index ON TABLE student;

SerDer:
=====
Serializer/Deserializer.
Holds the logic to convert unstructured data into records.
Implemented using Java.
Serializers are used while writing data and Deserializers are used at query time to execute SELECT statements.
Deserializer interface takes a string or binary representation of a record, and translates it into a Java object that Hive can manipulate.
Serializer, however, will take a Java object that Hive has been working with, and turn it into something that Hive can write to HDFS.


1. Load XML file into Hive.

input.xml
=======
<employee> <userid>1</userid> <name>Puneetha B M</name> <designation>Developer</designation> </employee>
<employee> <userid>2</userid> <name>Bhoomika</name> <designation>Analyst</designation> </employee>

create table xmlsample(xmldata string);

load data local inpath '/root/subha/input.xml' into table xmlsample;

CREATE TABLE xpath_table AS
SELECT xpath_int(xmldata,'employee/userid'),
xpath_string(xmldata,'employee/name'),
xpath_string(xmldata,'employee/designation')
FROM xmlsample;

select * from xpath_table;


2. Querying JSON Document:
=====================
input.json
=======

{"Foo":"ABC","Bar":"20090101100000","Quux":{"QuuxId":1234,"QuuxName":"Sam"}}

CREATE TABLE json_table ( json string );
 
LOAD DATA LOCAL INPATH  '/root/subha/input.json' INTO TABLE json_table;

The get_json_object takes two arguments: tablename.fieldname and the JSON field to parse, where '$' represents the root of the document.

Select get_json_object(json_table.json, '$') from json_table; 

Set hive.cli.print.header=true 

Select get_json_object(json_table.json, '$.Foo') as foo, 
       get_json_object(json_table.json, '$.Bar') as bar,
       get_json_object(json_table.json, '$.Quux.QuuxId') as qid,
       get_json_object(json_table.json, '$.Quux.QuuxName') as qname
from json_table;


CREATE  TABLE json_serde (Foo string,Bar string,Quux struct<QuuxId:int, QuuxName:string>)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.JsonSerde';

LOAD DATA LOCAL INPATH '/root/subha/input.json' INTO TABLE json_serde;

SELECT Foo, Bar, Quux.QuuxId, Quux.QuuxName
FROM json_serde;


UDF:
----
Write Java Program and convert it into jar

package com.example.hive.udf;

import org.apache.hadoop.hive.ql.exec.Description;
import org.apache.hadoop.hive.ql.exec.UDF;

@Description(
  name="SimpleUDFExample")
 public final class MyUpperCase extends UDF {
   public String evaluate(final String word) {
    return word.toUpperCase();
  }
}
2. Go to Hive Prompt
3. ADD JAR /root/subha/UpperCase.jar;
4. CREATE TEMPORARY FUNCTION touppercase AS 'com.example.hive.udf.MyUpperCase';
5. Select touppercase(repname) from SalesRep;

As of 0.13.0 you can make UDFs permanent through

ORC File Format
---------------

The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data. It was designed to overcome limitations of the other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data.

CREATE TABLE employee (EmployeeID Int,FirstName String,Designation String,Salary Int,Department String) CLUSTERED BY(EmployeeID) into 2 BUCKETS STORED AS ORC TBLPROPERTIES('transactional'='true');

insert into table employee values (1001,'John','TA',50000,'RCL'),(1002,'Jack','SSE',25000,'CLOUD'),(1003,'James','PM',100000,'RCL');

select * from employee;

update employee set salary=30000 where employeeid=1002;

select * from employee;

delete from employee where employeeid=1002;

select * from employee;
---------------------------------------------------------------------------

CONDITIONAL FUNCTIONS:

create table test(a int,b int);

insert into test values(10,20);

insert into table test values(20,30);

insert into table test values(30,40);

select a, IF(b=10,'Ten','Not Ten') from test;
-----------------------------------------------------------------------------
create table emp(name string,salary int);

insert into table emp values('Jack',2000),('John',5000),('James',10000),('Smith',20000),('Scott',15000);

SELECT name, salary,
CASE
WHEN salary < 5000.0 THEN 'low'
WHEN salary >= 5000.0 AND salary < 10000.0 THEN 'middle'
WHEN salary >= 10000.0 AND salary < 15000.0 THEN 'high'
ELSE 'very high'
END AS bracket FROM emp;
-----------------------------------------------------------------------
Multi Table Insert:
----------------------
10,1
20,2
30,3
40,4
50,5

CREATE TABLE input_a(a INT, KEY INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

LOAD DATA LOCAL INPATH '/hivedemo/input_a.txt' INTO TABLE input_a;

100,1
200,2
300,3
400,4
500,5

CREATE TABLE input_b(b INT, KEY INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

LOAD DATA LOCAL INPATH '/hivedemo/input_b.txt' INTO TABLE input_b;

CREATE TABLE output_a(a INT);
CREATE TABLE output_b(b INT);


FROM (
  SELECT a, b
    FROM input_a
    JOIN input_b ON input_a.KEY = input_b.KEY
) INPUT
INSERT OVERWRITE TABLE output_a
SELECT DISTINCT a
INSERT OVERWRITE TABLE output_b
SELECT DISTINCT b;

---------------------------------------------------------------------------------------

Union Clause:
-------------
combine the result sets of multiple queries. 
default => esult sets are combined as if the DISTINCT operator was applied.

syntax:
-------
query_1 UNION [DISTINCT | ALL] query_2

The UNION keyword by itself is the same as UNION DISTINCT. 

Example:
-------

create table few_ints (x int) stored as ORC;

insert into few_ints values (1), (1), (2), (3);

select x from few_ints order by x;

select x from few_ints union all select x from few_ints;

select x from few_ints union all select 10;

--------------------------------------------------
union => supported in hive 1.2

select x from few_ints union select x+1 from few_ints;

select x from few_ints union select 10;
-------------------------------------------------------------------------------------------------

Support for specifying column list in insert statement:
------------------------------------------------------

create table y (x int, y int);

insert into y (x) values(10);

select * from y;
--------------------------------------------------------------------------------------------------

http://hortonworks.com/blog/orcfile-in-hdp-2-better-compression-better-performance/

--------------------------------------------------------------------------------------------------
vectorization:
--------------
Vectorized query execution improves performance of operations like scans, aggregations, filters and joins, by performing them in batches of 1024 rows at once instead of single row each time.

Introduced in Hive 0.13.

Two parameters needs to be set:

set hive.vectorized.execution.enabled = true;
set hive.vectorized.execution.reduce.enabled = true;
--------------------------------------------------------------------------------------------------


